{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#download nltk libraries\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "word_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honetly idkI dont know what im even doing here...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Trigger warning] Excuse for self inflicted bu...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It ends tonight.I can’t do it anymore. \\nI quit.</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Everyone wants to be \"edgy\" and it's making me...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My life is over at 20 years oldHello all. I am...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3          i need helpjust help me im crying so hard      suicide\n",
       "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide\n",
       "5  Honetly idkI dont know what im even doing here...      suicide\n",
       "6  [Trigger warning] Excuse for self inflicted bu...      suicide\n",
       "7   It ends tonight.I can’t do it anymore. \\nI quit.      suicide\n",
       "8  Everyone wants to be \"edgy\" and it's making me...  non-suicide\n",
       "9  My life is over at 20 years oldHello all. I am...      suicide"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import dataset\n",
    "dataset = pd.read_csv('suicide.csv')\n",
    "\n",
    "#split dataset into three\n",
    "split_dataset = np.array_split(dataset,3)\n",
    "dataset = split_dataset[0]\n",
    "\n",
    "#drop \"Unnamed column from dataset\"\n",
    "dataset = dataset.drop('Unnamed: 0', axis=1)\n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning: Removing Stopwords,Tokenization,Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find missing values in the dataset\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in set(stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('class', axis=1)\n",
    "y = dataset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove emails\n",
    "email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "regex_to_remove = [email_regex, r'Subject:', r'Re:']\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    # removing all special character\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(X['text'][i]))\n",
    "    \n",
    "    #remove all urls\n",
    "    review = re.sub(r\"http?[A-Za-z0-9]+\", \"\", str(X['text'][i]))\n",
    "\n",
    "    #timestamps and date\n",
    "    review = re.sub(r'(2[0-3]|[01][0-9]|[0-9]):([0-5][0-9]|[0-9]):([0-5][0-9]|[0-9])',\"\",str(X['text'][i]))\n",
    "    review = re.sub(r\"^([1-9]|0[1-9]|1[0-9]|2[0-9]|3[0-1])(\\.|-|/)([1-9]|0[1-9]|1[0-2])(\\.|-|/)([0-9][0-9]|19[0-9][0-9]|20[0-9][0-9])$\",\"\",str(X['text'][i]))\n",
    "\n",
    "\n",
    "   # make document as lowerCase\n",
    "    review = review.lower()\n",
    "    # splitting the documents into words for ex ['iam', 'omar']\n",
    "    review = review.split()\n",
    "    # perfrom  lemmatization \n",
    "    review = [word_lemmatizer.lemmatize(word) for word in review if not word in stop_words]\n",
    "    # join the document again\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # removing emails\n",
    "    for r in regex_to_remove:\n",
    "        X['text'][i] = re.sub(r, '', review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Tf-IDF and Tf-IDF with 2 grams of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_vectorizer_n12 = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train['text'])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test['text'])\n",
    "\n",
    "X_tfidf_train_n12= tfidf_vectorizer_n12.fit_transform(X_train['text'])\n",
    "X_tfidf_test_n12=tfidf_vectorizer_n12.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "  \n",
    "X_bow_train = vectorizer.fit_transform(X_train['text'])\n",
    "X_bow_test = vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Sentiment Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "def get_vader_scores(data):\n",
    "    sid = SIA()\n",
    "    vader_df=data.copy()\n",
    "    vader_df['scores'] = vader_df['text'].apply(lambda txt: sid.polarity_scores(str(txt)))\n",
    "    \n",
    "    vader_df['neg_score'] = vader_df['scores'].apply(lambda txt: txt['neg'])\n",
    "    vader_df['neu_score'] =vader_df['scores'].apply(lambda txt: txt['neu'])\n",
    "    vader_df['pos_score'] = vader_df['scores'].apply(lambda txt: txt['pos'])\n",
    "    vader_df['compound'] = vader_df['scores'].apply(lambda txt: txt['compound'])\n",
    "    vader_df.drop('scores', axis=1, inplace=True)\n",
    "    vader_df.drop('text', axis=1, inplace=True)\n",
    "    return vader_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vader_train = get_vader_scores(X_train)\n",
    "X_vader_test= get_vader_scores(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# We Can select any model but linearSVC has l1 norm penality which deals with sparse\n",
    "lsvc = LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(X_tfidf_train, y_train)\n",
    "\n",
    "# This function select the best features that has high weigh\n",
    "fs = SelectFromModel(lsvc, prefit=True)\n",
    "# This function redeuce X to the selected features\n",
    "X_selection = fs.transform(X_tfidf_train)\n",
    "X_test_selection = fs.transform(X_tfidf_test)\n",
    "\n",
    "\n",
    "lsvc.fit(X_tfidf_train_n12, y_train)\n",
    "fs_n12 = SelectFromModel(lsvc, prefit=True)\n",
    "X_selection_n12 = fs_n12.transform(X_tfidf_train_n12)\n",
    "X_test_selection_n12 = fs_n12.transform(X_tfidf_test_n12)\n",
    "\n",
    "lsvc.fit(X_bow_train, y_train)\n",
    "fs_n12 = SelectFromModel(lsvc, prefit=True)\n",
    "X_selection_bow = fs_n12.transform(X_bow_train)\n",
    "X_test_selection_bow = fs_n12.transform(X_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(X_selection, y_train)\n",
    "y_predict_tfidf = lsvc.predict(X_test_selection)\n",
    "\n",
    "lsvc.fit(X_selection_n12,y_train)\n",
    "y_predict_tfidf_n12 = lsvc.predict(X_test_selection_n12)\n",
    "\n",
    "lsvc.fit(X_selection_bow,y_train)\n",
    "y_predict_bow = lsvc.predict(X_test_selection_bow)\n",
    "\n",
    "lsvc.fit(X_vader_train,y_train)\n",
    "y_predict_vader = lsvc.predict(X_vader_test)\n",
    "\n",
    "linear_svm_tfidf_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf)\n",
    "linear_svm_tfidf_n12_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf_n12)\n",
    "linear_svm_bow_results=metrics.precision_recall_fscore_support(y_test, y_predict_bow)\n",
    "vader_svm_results=metrics.precision_recall_fscore_support(y_test, y_predict_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_acc= metrics.accuracy_score(y_test, y_predict_tfidf)\n",
    "tfidf_n12_acc=accuracy_score(y_test, y_predict_tfidf_n12)\n",
    "bow_acc= accuracy_score(y_test, y_predict_bow)\n",
    "vader_acc=accuracy_score(y_test, y_predict_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'],\n",
    "         ['precision',linear_svm_tfidf_results[0][0],linear_svm_tfidf_n12_results[0][0],linear_svm_bow_results[0][0],\n",
    "          vader_svm_results[0][0]],\n",
    "         ['recall',linear_svm_tfidf_results[1][0],linear_svm_tfidf_n12_results[1][0],linear_svm_bow_results[1][0],\n",
    "          vader_svm_results[1][0]],\n",
    "         ['F1-score',linear_svm_tfidf_results[2][0],linear_svm_tfidf_n12_results[2][0],linear_svm_bow_results[2][0],\n",
    "          vader_svm_results[2][0]],\n",
    "        ['accuracy',tfidf_acc,tfidf_n12_acc,bow_acc,\n",
    "          vader_acc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤══════════╤═══════════════════╤════════════════╤══════════╕\n",
      "│           │   TF-IDF │   TF-IDF 2-grams  │   bag of words │    vader │\n",
      "╞═══════════╪══════════╪═══════════════════╪════════════════╪══════════╡\n",
      "│ precision │ 0.865235 │          0.866395 │       0.866266 │ 0.672957 │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ recall    │ 0.850177 │          0.843629 │       0.881192 │ 0.688205 │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ F1-score  │ 0.857639 │          0.854861 │       0.873665 │ 0.680496 │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ accuracy  │ 0.858842 │          0.85673  │       0.872544 │ 0.676792 │\n",
      "╘═══════════╧══════════╧═══════════════════╧════════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(data1,headers='firstrow',tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10)\n",
    "clf.fit(X_selection, y_train)\n",
    "y_predict_tfidf_2 = clf.predict(X_test_selection)\n",
    "\n",
    "clf.fit(X_selection_n12, y_train)\n",
    "y_predict_tfidf_n12_2 = clf.predict(X_test_selection_n12)\n",
    "\n",
    "clf.fit(X_selection_bow, y_train)\n",
    "y_predict_bow_2 = clf.predict(X_test_selection_bow)\n",
    "\n",
    "clf.fit(X_vader_train, y_train)\n",
    "y_predict_vader_2 = clf.predict(X_vader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_tfidf_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf_2)\n",
    "RandomForest_tfidf_n12_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf_n12_2)\n",
    "RandomForest_bow_results=metrics.precision_recall_fscore_support(y_test, y_predict_bow_2)\n",
    "RandomForest_vader_results=metrics.precision_recall_fscore_support(y_test, y_predict_vader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_tfidf_acc= metrics.accuracy_score(y_test, y_predict_tfidf_2)\n",
    "RandomForest_tfidf_n12_acc=accuracy_score(y_test, y_predict_tfidf_n12_2)\n",
    "RandomForest_bow_acc= accuracy_score(y_test, y_predict_bow_2)\n",
    "RandomForest_vader_acc=accuracy_score(y_test, y_predict_vader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'],\n",
    "         ['precision',RandomForest_tfidf_results[0][0],RandomForest_tfidf_n12_results[0][0],RandomForest_bow_results[0][0],\n",
    "          RandomForest_vader_results[0][0]],\n",
    "         ['recall',RandomForest_tfidf_results[1][0],RandomForest_tfidf_n12_results[1][0],RandomForest_bow_results[1][0],\n",
    "          RandomForest_vader_results[1][0]],\n",
    "         ['F1-score',RandomForest_tfidf_results[2][0],RandomForest_tfidf_n12_results[2][0],RandomForest_bow_results[2][0],\n",
    "          RandomForest_vader_results[2][0]],\n",
    "        ['accuracy',RandomForest_tfidf_acc,RandomForest_tfidf_n12_acc, RandomForest_bow_acc,\n",
    "          RandomForest_vader_acc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤══════════╤═══════════════════╤════════════════╤══════════╕\n",
      "│           │   TF-IDF │   TF-IDF 2-grams  │   bag of words │    vader │\n",
      "╞═══════════╪══════════╪═══════════════════╪════════════════╪══════════╡\n",
      "│ precision │ 0.790608 │          0.794895 │       0.780578 │ 0.769284 │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ recall    │ 0.903679 │          0.909537 │       0.907211 │ 0.768157 │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ F1-score  │ 0.843371 │          0.848361 │       0.839144 │ 0.76872  │\n",
      "├───────────┼──────────┼───────────────────┼────────────────┼──────────┤\n",
      "│ accuracy  │ 0.832127 │          0.837384 │       0.826051 │ 0.76883  │\n",
      "╘═══════════╧══════════╧═══════════════════╧════════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(data2,headers='firstrow',tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression,  SGDClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\")\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True)\n",
    "sgd = SGDClassifier(alpha=.0001, max_iter=50, loss='log',\n",
    "                                       penalty=\"elasticnet\", n_jobs=-1)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_selection, y_train)\n",
    "y_predict_tfidf_4 = voting_clf.predict(X_test_selection)\n",
    "\n",
    "voting_clf.fit(X_selection_n12, y_train)\n",
    "y_predict_tfidf_n12_4 = voting_clf.predict(X_test_selection_n12)\n",
    "\n",
    "voting_clf.fit(X_selection_bow, y_train)\n",
    "y_predict_bow_4 = voting_clf.predict(X_test_selection_bow)\n",
    "\n",
    "voting_clf.fit(X_vader_train, y_train)\n",
    "y_predict_vader_4 = voting_clf.predict(X_vader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tfidf_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf_4)\n",
    "en_tfidf_n12_results=metrics.precision_recall_fscore_support(y_test, y_predict_tfidf_n12_4)\n",
    "en_bow_results=metrics.precision_recall_fscore_support(y_test, y_predict_bow_4)\n",
    "en_vader_results=metrics.precision_recall_fscore_support(y_test, y_predict_vader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tfidf_acc= metrics.accuracy_score(y_test, y_predict_tfidf_4)\n",
    "en_tfidf_n12_acc=accuracy_score(y_test, y_predict_tfidf_n12_4)\n",
    "en_bow_acc= accuracy_score(y_test, y_predict_bow_4)\n",
    "en_vader_acc=accuracy_score(y_test, y_predict_vader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3= [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'],\n",
    "        ['precision',en_tfidf_results[0][0],en_tfidf_n12_results[0][0],en_bow_results[0][0],\n",
    "          en_vader_results[0][0]],\n",
    "         ['recall',en_tfidf_results[1][0],en_tfidf_n12_results[1][0],en_bow_results[1][0],\n",
    "          en_vader_results[1][0]],\n",
    "         ['F1-score',en_tfidf_results[2][0],en_tfidf_n12_results[2][0],en_bow_results[2][0],\n",
    "          en_vader_results[2][0]],\n",
    "       ['accuracy',en_tfidf_acc,en_tfidf_n12_acc, en_bow_acc,\n",
    "          en_vader_acc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(data3,headers='firstrow',tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_selection, y_train)\n",
    "y_pred_5 = bag_clf.predict(X_test_selection)\n",
    "\n",
    "bag_clf.fit(X_selection_n12, y_train)\n",
    "y_pred_n12_5 = bag_clf.predict(X_test_selection_n12)\n",
    "\n",
    "bag_clf.fit(X_selection_bow, y_train)\n",
    "y_pred_bow_5 = bag_clf.predict(X_test_selection_bow)\n",
    "\n",
    "bag_clf.fit(X_vader_train, y_train)\n",
    "y_pred_vader_5 = bag_clf.predict(X_vader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_tfidf_results=metrics.precision_recall_fscore_support(y_test, y_pred_5)\n",
    "bag_tfidf_n12_results=metrics.precision_recall_fscore_support(y_test, y_pred_n12_5)\n",
    "bag_bow_results=metrics.precision_recall_fscore_support(y_test, y_pred_bow_5)\n",
    "bag_vader_results=metrics.precision_recall_fscore_support(y_test, y_pred_vader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_tfidf_acc= metrics.accuracy_score(y_test, y_pred_5)\n",
    "bag_tfidf_n12_acc=accuracy_score(y_test, y_pred_n12_5)\n",
    "bag_bow_acc= accuracy_score(y_test, y_pred_bow_5)\n",
    "bag_vader_acc=accuracy_score(y_test, y_pred_vader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4= [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'],\n",
    "        ['precision',bag_tfidf_results[0][0],bag_tfidf_n12_results[0][0],bag_bow_results[0][0],\n",
    "          bag_vader_results[0][0]],\n",
    "         ['recall',bag_tfidf_results[1][0],bag_tfidf_n12_results[1][0],bag_bow_results[1][0],\n",
    "          bag_vader_results[1][0]],\n",
    "         ['F1-score',bag_tfidf_results[2][0],bag_tfidf_n12_results[2][0],bag_bow_results[2][0],\n",
    "          bag_vader_results[2][0]],\n",
    "        ['accuracy',bag_tfidf_acc,bag_tfidf_n12_acc, bag_bow_acc,\n",
    "          bag_vader_acc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(data4,headers='firstrow',tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Tweets form Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load environment variable\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys\n",
    "import os\n",
    "\n",
    "consumer_key= \"put your key here\"\n",
    "consumer_secret=\"put your key here\"\n",
    "access_token=\"put your key here\"\n",
    "access_token_secret=\"put your key here\"\n",
    "bearer_token = \"put your key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client( bearer_token=bearer_token, consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret,  wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing in my twitter API authentication keys\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret,access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the tweepy API\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query\n",
    "query = 'better off without me'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set time frame\n",
    "\n",
    "#from 2016\n",
    "start_time = '2016-01-01T00:00:00Z'\n",
    "\n",
    "#to 2021\n",
    "end_time = '2021-12-31-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain maximum 100 tweets for query for thepast 7 days\n",
    "response =  client.search_recent_tweets(query=query,expansions=['author_id'], max_results = 100, user_fields=[\"location\",\"username\",\"id\"], place_fields = [\"country\"], tweet_fields=['created_at',\"lang\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract names, usernames, tweet_ids and tweets from the response\n",
    "usernames = []\n",
    "tweets = []\n",
    "location =[]\n",
    "\n",
    "for tweet in response.data:\n",
    "    for details in response.includes['users']:\n",
    "        usernames.append( details.username)\n",
    "        location.append(details.location)\n",
    "        tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Username</th>\n",
       "      <th>Profile_Image_URL</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Language</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SJKHC83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I still have feelings for you &amp;amp; no matter ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YOUHAVENONIPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I still have feelings for you &amp;amp; no matter ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cupidflu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I still have feelings for you &amp;amp; no matter ...</td>\n",
       "      <td>honami . . . project sekai!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jbwhittaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I still have feelings for you &amp;amp; no matter ...</td>\n",
       "      <td>Strongsville, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adhdistic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I still have feelings for you &amp;amp; no matter ...</td>\n",
       "      <td>hd. zzipa tumblr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8923</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>regretfulbot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know if anyone considers me to be a fr...</td>\n",
       "      <td>quotebot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>harmmms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know if anyone considers me to be a fr...</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8925</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iamstephyg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know if anyone considers me to be a fr...</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8926</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watcherskyduo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know if anyone considers me to be a fr...</td>\n",
       "      <td>©pouistired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nowhere_Matt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know if anyone considers me to be a fr...</td>\n",
       "      <td>Central Coast | Darkinjung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8928 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_ID Name       Username Profile_Image_URL Tweet_ID Language  \\\n",
       "0        NaN  NaN        SJKHC83               NaN      NaN      NaN   \n",
       "1        NaN  NaN  YOUHAVENONIPS               NaN      NaN      NaN   \n",
       "2        NaN  NaN       cupidflu               NaN      NaN      NaN   \n",
       "3        NaN  NaN    jbwhittaker               NaN      NaN      NaN   \n",
       "4        NaN  NaN      adhdistic               NaN      NaN      NaN   \n",
       "...      ...  ...            ...               ...      ...      ...   \n",
       "8923     NaN  NaN   regretfulbot               NaN      NaN      NaN   \n",
       "8924     NaN  NaN        harmmms               NaN      NaN      NaN   \n",
       "8925     NaN  NaN     iamstephyg               NaN      NaN      NaN   \n",
       "8926     NaN  NaN  watcherskyduo               NaN      NaN      NaN   \n",
       "8927     NaN  NaN   Nowhere_Matt               NaN      NaN      NaN   \n",
       "\n",
       "                                                  Tweet  \\\n",
       "0     I still have feelings for you &amp; no matter ...   \n",
       "1     I still have feelings for you &amp; no matter ...   \n",
       "2     I still have feelings for you &amp; no matter ...   \n",
       "3     I still have feelings for you &amp; no matter ...   \n",
       "4     I still have feelings for you &amp; no matter ...   \n",
       "...                                                 ...   \n",
       "8923  I don't know if anyone considers me to be a fr...   \n",
       "8924  I don't know if anyone considers me to be a fr...   \n",
       "8925  I don't know if anyone considers me to be a fr...   \n",
       "8926  I don't know if anyone considers me to be a fr...   \n",
       "8927  I don't know if anyone considers me to be a fr...   \n",
       "\n",
       "                         Location  \n",
       "0                            None  \n",
       "1                            None  \n",
       "2     honami . . . project sekai!  \n",
       "3                Strongsville, OH  \n",
       "4                hd. zzipa tumblr  \n",
       "...                           ...  \n",
       "8923                     quotebot  \n",
       "8924                  everywhere   \n",
       "8925              Los Angeles, CA  \n",
       "8926                  ©pouistired  \n",
       "8927   Central Coast | Darkinjung  \n",
       "\n",
       "[8928 rows x 8 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a pandas dataframe for the obtained results\n",
    "tweet_df = pd.DataFrame(columns=[\"User_ID\", \"Name\", \"Username\", \"Profile_Image_URL\", \"Tweet_ID\",\"Language\",\"Tweet\"])\n",
    "tweet_df[\"Username\"] = usernames\n",
    "tweet_df[\"Tweet\"] = tweets\n",
    "tweet_df['Location'] = location\n",
    "\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_unique = pd.unique(tweet_df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_unique_df = pd.DataFrame(tweets_unique, columns=[\"Tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_tweets = tweets_unique_df.to_csv(\"tweets_phrase_better_off_without_me.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #fit data to a naive bayes classifier\n",
    "    classifier = RandomForestClassifier.train(X_train)\n",
    "\n",
    "    # the first tweet in our dataframe\n",
    "    example_tweet = tweet_df[\"Tweet\"].values[0]\n",
    "\n",
    "    #predict\n",
    "    print((example_tweet,classifier.classify(dict([token, True] for token in tweet_df))))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
